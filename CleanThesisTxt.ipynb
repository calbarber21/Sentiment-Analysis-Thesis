{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CleanThesisTxt.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO6jwUt251SHEzbqGLI/X8o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calbarber21/Sentiment-Analysis-Thesis/blob/master/CleanThesisTxt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "IWTnvog8RHTl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA1t-Evsbmug",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "914cd03e-94c7-480d-f4a6-75592d25f946"
      },
      "source": [
        "from io import StringIO\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from linecache import getline\n",
        "import time\n",
        "import datetime\n",
        "import sys\n",
        "import pandas as pd \n",
        "import pdb\n",
        "!pip install xlsxwriter\n",
        "import xlsxwriter\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import tokenize\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "#google collab specific line to connect to google drive\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n",
        "#change directoy to .txt files directory\n",
        "%cd /content/gdrive/My\\ Drive/PDF Data Thesis/Text Data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting xlsxwriter\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/98/17875723b6814fc4d0fc03f0997ee00de2dbd78cf195e2ec3f2c9c789d40/XlsxWriter-1.3.3-py2.py3-none-any.whl (144kB)\n",
            "\r\u001b[K     |██▎                             | 10kB 17.2MB/s eta 0:00:01\r\u001b[K     |████▌                           | 20kB 1.2MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 30kB 1.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 40kB 1.9MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 51kB 1.5MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 61kB 1.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 71kB 1.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 81kB 2.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 92kB 2.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 102kB 2.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 112kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 122kB 2.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 133kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 143kB 2.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 153kB 2.0MB/s \n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-1.3.3\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "4/4AFQ8QNU2vNqHY9ZV0aqx2xhyZ6c0UwHaGEhM8p_f-8odExyb5DxUhQ\n",
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/PDF Data Thesis/Text Data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parse Corpus"
      ],
      "metadata": {
        "id": "9PPlE6hHRFHp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5XXuezydWJM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "0d5edc63-7624-4a9b-c55b-0e04744d3b54"
      },
      "source": [
        "files=[i for i in os.listdir() if i.endswith('txt')]\n",
        "data = pd.DataFrame({'Title':[],'Date':[],'article':[]})\n",
        "for file in files :\n",
        "  fileX= open(file, \"r\")\n",
        "  text = fileX.read()\n",
        "  text = text.split(\"End of Document\")\n",
        "  count = 0\n",
        "  for episode in text :\n",
        "    if count > 0 :\n",
        "      newrow={'Title':title, 'Date':date,'article':body}\n",
        "      data=data.append(newrow, ignore_index=True)\n",
        "    getLines = False\n",
        "    body = []\n",
        "    episode = episode.splitlines()\n",
        "    episode = [episode for episode in episode if episode.strip() != '']\n",
        "    for i , line in enumerate(episode,start=0) :\n",
        "      line = line.strip()\n",
        "      if (\"The New York Times\" == line) : \n",
        "        if (i == 2) :\n",
        "          title = episode[i - 2].strip() + \" \" + episode[i - 1].strip()\n",
        "        else :\n",
        "          title = episode[i - 1].strip()\n",
        "        date = re.findall(r'(?:\\d{2} )?(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* (?:\\d{1,2}, )?\\d{4}',episode[i + 1])\n",
        "      if line.startswith(\"Body\") :\n",
        "        getLines = True\n",
        "      if getLines :\n",
        "        body.append(line)\n",
        "    count = count + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                   Title  ...                                            article\n",
            "0                           Botching the Great Recession  ...  [Body, It has been ten years since the failure...\n",
            "1      Rebuffed by Trump on Tariffs, Businesses Mount...  ...  [Body, WASHINGTON — They’ve testified, tweeted...\n",
            "2                   'Anonymous' Is Hiding in Plain Sight  ...  [Body, More and more, I wonder if the disgrunt...\n",
            "3      Economic Bellwether Is Making a Midwestern Cit...  ...  [Body, ELKHART, Ind. -- The tables were filled...\n",
            "4                       Paid Notice: Deaths BOVIN, HENRY  ...  [Body, BOVIN--Henry, 97, of Morristown, NJ die...\n",
            "...                                                  ...  ...                                                ...\n",
            "20694       No Region Safe As Cases Soar, Fauci Cautions  ...  [Body, The government's top infectious disease...\n",
            "20695          Optimism By Mnuchin Is Tempered By Powell  ...  [Body, The Treasury secretary and the Fed chai...\n",
            "20696  Biden Team Begins Raising Money for Transition...  ...  [Body, A small staff of advisers and other tru...\n",
            "20697  In Europe, Travel Returns, but Not Confidence ...  ...  [Body, Borders and businesses are reopening, e...\n",
            "20698                  Markets Are Called Too Complacent  ...  [Body, Tame markets do not mean the path back ...\n",
            "\n",
            "[20699 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expand Contractions"
      ],
      "metadata": {
        "id": "A4YVj0VWQ-IY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUbUbnvKWkTd"
      },
      "source": [
        "CONTRACTION_MAP = {\n",
        "\"ain't\": \"is not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"I'd\": \"I would\",\n",
        "\"I'd've\": \"I would have\",\n",
        "\"I'll\": \"I will\",\n",
        "\"I'll've\": \"I will have\",\n",
        "\"I'm\": \"I am\",\n",
        "\"I've\": \"I have\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'll've\": \"i will have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what'll've\": \"what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who'll've\": \"who will have\",\n",
        "\"who's\": \"who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you'll've\": \"you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zahPxBUMj2x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "57a77958-f65b-419b-c7e5-1d21ddc84f8c"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
        "    \n",
        "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
        "                                      flags=re.IGNORECASE|re.DOTALL)\n",
        "    def expand_match(contraction):\n",
        "        match = contraction.group(0)\n",
        "        first_char = match[0]\n",
        "        expanded_contraction = contraction_mapping.get(match)\\\n",
        "                                if contraction_mapping.get(match)\\\n",
        "                                else contraction_mapping.get(match.lower())                       \n",
        "        expanded_contraction = first_char+expanded_contraction[1:]\n",
        "        return expanded_contraction\n",
        "        \n",
        "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "    return expanded_text\n",
        "\n",
        "\n",
        "data_text = data[['article']]\n",
        "data_text['article'] = data_text['article'].apply(', '.join)\n",
        "\n",
        "documents = data_text\n",
        "processed_docs = documents['article'].map(expand_contractions)\n",
        "print(processed_docs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0        Body, It has been ten years since the failure ...\n",
            "1        Body, WASHINGTON — They’ve testified, tweeted,...\n",
            "2        Body, More and more, I wonder if the disgruntl...\n",
            "3        Body, ELKHART, Ind. -- The tables were filled ...\n",
            "4        Body, BOVIN--Henry, 97, of Morristown, NJ died...\n",
            "                               ...                        \n",
            "20694    Body, The governments top infectious disease e...\n",
            "20695    Body, The Treasury secretary and the Fed chair...\n",
            "20696    Body, A small staff of advisers and other trus...\n",
            "20697    Body, Borders and businesses are reopening, ev...\n",
            "20698    Body, Tame markets do not mean the path back t...\n",
            "Name: article, Length: 20699, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove Accented Characters"
      ],
      "metadata": {
        "id": "WZYO_7ATRQd8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THPvz3Qnl5wk"
      },
      "source": [
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text\n",
        "\n",
        "df = pd.DataFrame(processed_docs)\n",
        "data_text = df[['article']]\n",
        "#data_text['article'] = data_text['article'].apply(', '.join)\n",
        "\n",
        "documents = data_text\n",
        "processed_docs = documents['article'].map(remove_accented_chars)\n",
        "print(processed_docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove Special Characters"
      ],
      "metadata": {
        "id": "r-DgWe-cRTEG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unrk_Zs3gzXW"
      },
      "source": [
        "def remove_special_characters(text, remove_digits=False):\n",
        "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n",
        "df = pd.DataFrame(processed_docs)\n",
        "data_text = df[['article']]\n",
        "#data_text['article'] = data_text['article'].apply(', '.join)\n",
        "\n",
        "documents = data_text\n",
        "processed_docs = documents['article'].map(remove_special_characters)\n",
        "print(processed_docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatize"
      ],
      "metadata": {
        "id": "P35KTNEPRWfV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0J8oDJB_ohs"
      },
      "source": [
        "import gensim\n",
        "import unicodedata\n",
        "from gensim.utils import simple_preprocess\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "stopword_list.remove('no')\n",
        "stopword_list.remove('not')\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "import numpy as np\n",
        "np.random.seed(2018)\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "def lemmatize_stemming(text):\n",
        "      return WordNetLemmatizer().lemmatize(text, pos='v')\n",
        "def preprocess(text):\n",
        "      result = []\n",
        "      for token in gensim.utils.simple_preprocess(text):\n",
        "          if token not in stopword_list and len(token) > 2 and token != \"no\" and token != \"not\":\n",
        "              result.append(lemmatize_stemming(token))\n",
        "      return result\n",
        "\n",
        "df = pd.DataFrame(processed_docs)\n",
        "data_text = df[['article']]\n",
        "data_text['index'] = data_text.index\n",
        "documents = data_text\n",
        "processed_docs = documents['article'].map(preprocess)\n",
        "processed_docs = processed_docs.map(remove_accented_chars)\n",
        "processed_docs.to_csv('AllData_PreWeighted.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}